* TODO
** DONE generar un modelo en pgmodeler estilizado
- State "DONE"       from "TODO"       [2019-05-08 mié 15:04]
seria primero en 9.6 sin serial ni sequence solo primary key
eliminando los campos que no se usan.
generar el source en sql, editarlo y luego adosarlo en un escript con
pg_execute, para subirlo, con drops antes.
incluir los create functions.
Ya se hizo y funciona rapidamente, borra y recrea las tablas.
** DONE usaremos primarykey unicamente y generaremos el id por tcl
- State "DONE"       from "TODO"       [2019-05-09 jue 20:44]
las tablas sin el serial o el sequence permiten el llenado con csv en
el caso de barrios,zonas, en el caso de clientes dio un error ya en
los datos, que fue por datos de tipo money no aceptados en el formato
en que se presentaban.
El escript propio esta corriendo para clientes solo y va muy lento y
todavia no se si va a ser grabado. 
** [2019-05-08 mié] se produjo el pasado a mysql
al final con postgres tuvimos muchos inconvenientes a la hora de subir
los datos a cloud, (y tambien a la hora de pasar los datos de sqlite a
postgres convengamos)
** [2019-05-09 jue] mysql hasta ahora

*** creacion de instancia/usuario/db
*** lectura de dde por script tcl
*** script interno para generar tablas e importar csv (con errores)
*** subida a cloud de generacion de tablas por script tcl
*** importacion por csv (con errores) y por dump.
** TODO terminar de pasar las tablas para el dump
** TODO transcribir las funciones a mysql
** TODO transcribir el ventaspg.tcl a ventasmysql.tcl
** conexion a AWS con psql por terminal
pg_dump -f mydbdump.sq
psql  -f mydbdump.sql -h
rx2019.cxsjyjct3wug.us-east-2.rds.amazonaws.com -U hero -d  hero
pide ahi la clave del usuario hero que es ata y listo comienza el
restore/

* diary
** [2019-05-09 jue]
hoy un dia positivo dentro de todo pq he logrado algunos resultados
*** logre entrarle al ::mysql::tcl
casi de una pase el metodo llenarm y funciona incluso con una simpleza
mayor que los otros, un par de sentencias menos. es muy funcional el
wraper y le entre al map y al sel, por ahora.
*** pase casi directo sin problemas al llenado o volcado de datos de sqlite por tcl
casi se pasa directo lo que teniamos antes o sea que tendremos todo
rapido
me hizo renegar mucho el varchar limitado de clientes que quedaba
estrecho para los largos de campos, pero ya lo vamos a corregir.
*** como es el proceso
**** la tabla se crea rapidamente en workbech y de ahi se saca el sql y se lo pasa a tcl
**** en tcl se cura el script que sirve para crear tabla en localhost y en cloud
**** un script lee de sqlite3 los datos a localhost bastante rapido
**** por workbench hago un dump en un solo sql una sola transaccion instantaneo
**** ese solo archivo lo subo y lo importo en un toque en cloud
**** o sea no usaremos CSV sino tcl y dump, 
mucho mas rapido y manejable por mi, parte pq el csv es archivo por
archivo y el dump es general y el volcado lo hago por script y todo lo
que sea por escript por mas lerdo que sea se hace solo.
*** logre corregir el problema de las ñ 
que era un encoding que lo paso directo a tcl y me lee bien los datos.
*** Se podria decir que la estructura de trabajo ya esta lista, falta completar
falta terminar el resto de las tablas y subir todo junto y luego hacer
funcionar el pasador de ventas, y luego o antes quizas el tema de las
funciones.
** [2019-05-10 vie]
hoy un dia totalmente frustrante pq si bien es cierto que he logrado
ciertos avances que estaban cantados y eso en forma trabajosa veo
claroscuros en el programa y quedo totalmente trabado con el tema de
las funciones de forma tal que tengo que aprenderlo de cero y esto es
un tema crucial y muy engorroso para hacer y la funcion que
necesitamos es muy compleja.
Para poder hacer una prueba operativa del cloud tendria que poder
subir el sistema de archivos en la nube sin problemas y conectar el
cliente pasar ventas y poder pasar ventas sin problemas desde
cualquier maquina, En forma rapida y sin delay.
*** Al menos termino con el HITO tener todos los datos en la nube
tengo el script completo en tcl que permite rehacer las tablas
completamente en la nube por separado para evitar errores y ya lo voy
a unificar en localhost para tambien tener uno solo. Y emito por
workbench un solo dump que lo subo y lo lee sin problemas
rapidamente. O sea la subida de datos se haria sin complicaciones.
*** decido trabajar con el padron en forma local con sqlite3!!
mas alla de los problemas de mysql tambien en postgres era lerdo y por
lo tanto creo que no vale la pena. se puede tener una copia local en
toda maquina facilmente y consultar ahi. no hay problema con eso.
*** me queda por una parte el tedioso tema de estudiar funciones y triggers
*** por otro hacer la transcripcion del ventaspg a ventasmysql
eso hare mañana parte que es mas facil y lograble y parte pq es mas
util ya que lo que necesito es lograr saber lo antes posible si
podemos contar con un cliente que logre hacer algo mas complejo desde
el cloud. De ultima se podria hacer una pmovto boba que retorne la
fecha hasta que tengamos la verdadera.
** [2019-05-11 sáb]
a pesar de la enorme frustracion de no poder subir un trigger al cloud
y querer tirar todo por la borda, pienso que no debo pensar todo en
funcion del cloud.
de ultima tener todo hecho en funcion de mysql que es como un
estandard mas generalizado y seguir adelante con los clientes en tcl
que son los que conozco y alli tengo dos cosas positivas para
resaltar: 
*** que logre la funcion pmovto!!
e incluso hice otra funcion ultpago, y de paso hice todos los trigger
que son mas sencillos y faciles de editar y mantener. Incluso para
ello creo que php da la plataforma mas facil para editar y mantener a
pesar de la fea letra.
*** que anoche en los videitos vi como se puede unir mysql con boostrap para hacer tablas en html
o sea que realmente mysql es un standart y que tener todo aca no es
perdida.
que pasarla al cloud se podra eventualmente si o si en el futuro.
no hay que perder tiempo ahora en eso. hay que seguir con lo de abajo.
Ahora falta pasar el programa de ventas.
*** lo que paso en el marques
llegue a la conclusion que el sucinto mytclsql no servia para generar
un array en forma tan clara como pgtcl lo cual me dejaba a pata para
una situacion gui por lo cual ante la imposibilidad de emularlo por
proc decidi pasarme a tdbc::mysql y en esto estaba en la
transformacion de ventaspg a ventasmysql lo cual no era facil en el
bullicio y alli apago la maquina.
Me pongo a leer el documento de tdbc y alli se me ocurre leer sobre
AWS y bingo alli mismo veo que funciona en argentina y parece ser
mejor que google aunque un poco mas caro pasado los doce meses.
** [2019-05-12 dom]
*** inicio de AWS creacion de instancia vuelta a postgres
la creacion de la cuenta en AWS fue muy facil y la instancia en
postgres tambien, y el subido de datos muy facil ya que se puede hacer
directamente desde la terminal local via psql poniendo el host
remoto. o sea una papita. Solo tuve que corregir el campo money que da
eror pq es algo que depende de los locale y al ir a la nube va a dar
error y sacar el procedure pq antes de la version 11 no existia, cosa
que no sabia, fuera de eso, pude cargar perfectamente bien los datos.
*** carga de datos sin problemas
solo haciendo dump y haciendo restore desde la terminal en forma muy
simple y rapida y en forma local.
y sube todo, funciones, triggers, datos todo.
Faltaria hacer un script de borrado/vaciado para corrrer cada vez que
necesite resubir la base para volver a cargar los datos frescos.
*** la aplicacion de ventas funciona perfecto en la nube
integrandose perfectamente con el padron con sqlite es mas en forma
ultraveloz con el padron pq sqlite usa fts. La conexion inicial tiene
un pequeño delay pero luego la carga de la operacion es instantanea,
como si tuvieramos en local. y la vision de la tabla de atras que es
calculada a demanda tambien, o sea es el pg4 el demorero.
*** o sea que ahora a meterle pata con todo a la programacion con el cronograma original
reescribir las grandes interfaces conectadas y perfeccionar los datos
para que tengamos lo antes posible las cosas en forma lo mas acabada
posible.
** [2019-05-13 lun]
hoy di un paso gigante pq compre el monitor y traje el teclado o sea
arme una casi compu en la casa lo que permite una mejor
programacion. Y resolvi los tres problemas que tenia de ayer, y avance
bastante en la escritura de triggers los cuales hago sobre la marcha
con omni. ahora voy a pasarlos a script para ordenar las reescrituras
sino se va a hacer un lio.
*** gigante OMNIDB para escribir funciones y triggers
te permite rapidamente escribir ahi mismo y crearlos y con un terminal
de pruebas aparte con un par de consultas voy probando y testeando;
*** a partir de ahora CUADERNO MACHETE 
la bocha seria pasar las fichas cuando vaya pudiendo. E ir generando
una especie de indice en las ultimas paginas cosa de encontrar mas
rapido las cosas. Me parece mejor que las fichas y a la larga mas
barato. De ultima le voy metiendo lapiz y goma. 
*** gigante el pgmodeler que me lee el modelo de aws importandolo
y luego haciendo diff en local.
Cosa de que puedo trabajar en aws y bajar los cambios y sincronisarlos
via pgmodeler teniendo el modelo como la base para evitar tener que
hacer escript de actualizacion a mano que son error prone. Asi puedo
modificar funciones y trigger a gusto incluso tablas y luego acomodar
los modelos aca. El unico escript que tengo que arreglar es el de
lectura de datos de sqlite3 para actualizar los datos reales.
** [2019-05-14 mar]
por lo menos termino bien, he empezado bien el pasador de recibos y
creo que ira rapido, en lo que mas se pierde tiempo en en el modelo de
datos.
Hoy se me fue casi toda la tarde en agregar trigger, corregir
funciones, agregar dos tablas, agregar un tab que faltaba al pasador
de ventas y en hacer una ronda de pasado de datos que no fue sin
problemas, sino que saltaron los problemas en todos lados y los fui
solucionando, tanto del owner de la base, como de no poder limpiar
correctamente el destino de los datos. Y de paso documento bien el
proceso y tengo que hacerlo a menudo para ir haciendolo de memoria pq
lo haremos varias veces hasta que quede fijo.
** [2019-05-15 mié]
hoy dire que esto avanza con una lentitud espantosa, voy bocetando el
pasar pagos yendo a la simpleza y rozando la complejidad del F9
totalmente funcional y acabado como una roca y empezar de nuevo desde
el llano algo pretendiendo que sea simple y empezar a enrollarlo de
nuevo sin demasiada genialidad puesta de manifiesto en realidad no
logra nada nuevo, solo una reescritura que permita que funcione con el
nuevo sistema en vez de tener que hacer un wrap que al final es mas
dificil, y quizas lograr algo como el ventaspg.
Lo que si tendremos que hacer es separar el pasado de los papeles de
la parte gerencial que siempre estuvo unido y no puede estarlo, no
puede estar el pasado de recibos con la lista de cobranzas del mes y
cosas asi. 
** [2019-05-16 jue]
el dia de hoy llevo 6 horas y media sentado en la compu, sin
levantarme, un dia que tengo un migral, un ibu y un diclo encima, y
estoy en una curva de aprendizaje tipo pared de acantilado. Muy
poquito lo que pude moverla. Para un dia casi nada. Desde el domingo
que el pasar ventas funcionaba o creo que funcionaba el lunes lo pase
en la compra del monitor y en el acomodado de la base de datos y el
martes-miercoles me trabe en el pasar recibos y eso me frustro lo
suficiente como para pensar que no era viable el proyecto con tcl. Y
que debiamos empezar todo de cero. Todo de la nada. 
Habria que reevaluar pienso, entre el valor de lo que tengo y la
viabilidad de lo que podria llegar a hacer y quizas el poder hacerlo
pero mas adelante.
** [2019-05-17 vie] 
al final volvi con todos los brios a la programacion con el entorno
TCL y anduve bastante bien teniendo en cuenta la limitacion de
horarios de hoy. Un poco tropece con un problema de pgmodeler que me
borra los update de columnas y eso me hizo renegar dandome error en la
base local, hasta que descubri el error, y luego que no me andaba
pgadmin4 pq habia trasteado con apache2 hasta que borre todas las
config agregadas y felizmente volvio a a ndar pq en realidad pgadmin4
es fundamental para trabajar con postgres.
y en el medio me surgio la ultima gran duda que es la compatibilidad
con windows, y el poder pasar a trabajar con tdbc en vez de pg, hasta
qeu perdiendo una hora mas en eso me di cuenta que es al pedo, capaz
que reniego como negro en eso y no obtengo nada en limpio, lo mismo
termina andando para la mierda el sistema y nos trabamos al pedo
total.
A la noche en el aburrimiento leo mas sobre postgres y aprendo sobre
el pg_dump y a la mañana me instalo el adminer.
** [2019-05-18 sáb]
en cierta forma va paralelo el desarrollo de la interfase con el
manejo de la base de datos pq si no domamos los datos no podemos hacer
nada, tengo que ser experto en postgres para poder con solvencia
pasarme.
** [2019-05-19 dom]
descubri que una instancia en saopaolo es mucho mas rapida, y tambien
que cloud google puede ser usado via psql en igual manera que aws y
pude en dos segundos subir un dump y hacer funcionales mis programas
desde la consola con un psql -f en una forma muy compacta, ya que
desde consola google permite establecer basedatos y user entonces de
una subimos, en dos minutos la tenia configurada y es postgres 11 beta
ya. Lo unico que la facturacion va galopando y creo que va a superar
los 30 por mes uses o no uses el servicio.
Adminer anda joyisima en el sistema y algunos css son buenos y
permiten trabajar con las tres bases al mismo tiempo. la creacion de
tablas es la mas rapida y directa aca.
** [2019-05-20 lun]
hoy fue fluido pq me permiti copiar procesos enteros que estan bien en
otro lado, o sea verifico la aplicacion anterior y si veo que un tab
esta correcto o funciona bien y es adecuado, lo copio y lo
adapto. Entonces gano tiempo. P.e. en una tarde cerre el recibos con
dos tabs que faltaban. La que queda de las zonas ira en otro lado, y
el resumen de planillas ira con stock, con el buscador y con el fichas
voy a demorar mas pq son dos aplicaciones mas complejas pero en si son
mas modernas por lo que quizas tenga que directamente copiarlas parte
por parte e irlas mejorando a medida que las transformo.
** [2019-05-21 mar]
hoy comence de una la transcripcion del fichas a fichaspg y fue
bastante rapido, creo que en cuestion de un par de dias lo tendre
terminado, no ya de un par de semanas. y mas rapido voy con las nuevas
funciones que he creado
pg_list / pg_onecolumn / pg_exists 
Creo que asi esta bien , al menos con las interfaces que andan bien, y
luego si se da se podrian perfeccionar, pero no es necesario
reescribir algo que funciona bien.
** [2019-05-23 jue]
muchos avances con el fichaje, casi todos los modulos importantes, ya
funcionan, solo faltan las ventanas emergentes que creo que seran
faciles.
Y el gran problema descubierto ya al menos es la impresion que se
ralentiza pq es una lectura sucesiva, o sea esta programada para
trabajar en base de datos local y por lo tanto sobre la base de leer a
cada rato y eso tiene una penalidad de tiempo enorme en la emision de
un listado, un segundo por cliente, pq una lectura es una lectura sea
un datito o una tonelada de datos, o sea que lo que tengo que hacer
pienso es leer todo y procesarlo internamente con tcl y mandarlo a
imprimir en unos breves segundos. Quizas necesite usar la tabla cuotas
con sus trigger bien aceitados para que nos manejemos con eso en vez
de la forma tradicional. Pero en solo tres dias uno de los cuales ayer
fue a medias por problemas ajenos a la programacion traduje uno de los
principales y diria yo mas desafiantes. Queda el buscador que es el
otro grande y el resto creo que es ya mas facil. 
** [2019-05-25 sáb]
hoy fue un dia de altibajo, tanto que en el bajo pense en buscar otra
cosa o en abandonar el proyecto. Y las lecturas fluyeron de nuevo por
todos lados, por el mundo python (sqlalchemy y toda esa mierda).
incluso otros gui de python.
Todo motivado pq la impresion no me daba la velocidad requerida o sea
demoraba unos insaceptables segundos.
Y el tema es que todo lo que es conexion recurrente es lento, lo lento
en la nube es la coneccion repetitiva. Como el listado es lo mas
intensivo produce problemas. 
Lo solucione con una lectura inicial de todo, el pasado a tablas
temporales de sqlite en una base de datos en memory y luego el proceso
heavy dutty con sqlite como local sin tener que leer en postgres y
alli emito el listado luego para las insersiones repetitivas las
soluciono con un foreach agolpandolas en un solo insert y voile!! en
un segundo esta.
Con esto funcionando no tengo necesidad por ahora de irme a otros
temas pq para lograr lo que tengo aca necesitaria un desarrollo de
mucho tiempo.
** [2019-05-27 lun]
hoy hice buen avance pq me quedo redondito el autorizador ya listo pq
estaba programado en tdbc y es casi directo el paso asi que me quedo
asi, con la esperanza incluso que ande directo en windows.
Hice un volcado de datos, un pg_dump y una subida impecable tanto a
saopaulo como a aws (solo con el problema del procedure por postgres
11), pero los datos andan bien y el volcado se hace sin problemas. 
Y hoy me dedique bastante a un tema muy engorroso y dificultoso que
son los triggers, vaya si hay un tema embolante es este, pero hay que
hacerlo algun dia y ese dia es hoy.
** [2019-05-28 mar]
hoy me dedique a trabajo de sqlite tuve que programar los listados
para sistema kangoo, de todos modos consegui reusar codigo de puta
madre y como aditamento logre una impresion por columnas muy prolija y
con relativo poco esfuerzo y muy clarita a nivel codigo. 
Creo que el proximo que agarrare sera el buscador para traducir paso
por paso.
** [2019-06-01 sáb]
sigo traduciendo buscadorpg en forma ordenada reproduciendo fielmente
su contenido. creo que asi esta bien. salvo por los tabs interiores
que los ire evaluando para ver si son pertinentes y los ire acomodando
y perfeccionando.
creo que es un trabajo de paciencia y creo que en un mes o mes y medio
ya estara listo todo. maximo dos meses. 
** [2019-06-03 lun]
di de baja la cuenta de AWS pq pareciera que empiezan a facturar sin
periodo de gracia, y para no arriesgar cerre la cuenta.
* problemas
** DONE update fechas en ventaspg
- State "DONE"       from "TODO"       [2019-05-13 lun 18:26]
no anda ninguna tecla. Era un anodino bug de tcl. Resulta que el bind
para update de tbls-fechas es para KP_Add y KP_Substract no para plus
ni para minus, o sea al estar en la notebook no andaba ni a palos
porque al no tener teclado numerico no le podia dar teclazo adecuado y
como me hacia sospechar de aws pensaba cualquier cosa. Lo puse muy
facilmente como un swich - arriba del otro sin siquiera tener que
repetir la orden.

** DONE trigger update ventas se hace recursivo
- State "DONE"       from "TODO"       [2019-05-13 lun 20:17]
al no haber especificidad como en sqlite se hace recursivo y no puedo
updatear una columna especifica .
parece que postgres es terminante con eso o sea no se puede hacer. es
problema de diseño. o sea los datos deberian poder calcularse sobre la
base de lo que hay no en forma de trigger. Hay que repensar el
diseño. que estaria mal hecho. hay que volver a usar el lapiz.
Solucionado pq hice triggers especificos gracias a update of, lo cual
actualiza los cambios pero no cae en un bucle infinito.
** DONE no poder agregar ip publicas fuera de la creadora
- State "DONE"       from "TODO"       [2019-05-13 lun 15:33]
no pude agregar tan facil las ip de la oficina y del dpto.
el reniegue fue que habia que agregar la ip en realidad se agrega mas
facil que google pero en el security group del momento de la creacion,
que a su vez es el que aparece en la portada de la instancia, o sea
que no hay que crear otros sino que hay que agregarlo a ese, lo que
confunde es que no tiene un nombre muy potable. Tiene que aparecer en
la portada como rules ahi a la vista en el listado la ip que agregamos.
** DONE no me deja delete un registro en la tabla pagos
- State "DONE"       from "TODO"       [2019-05-14 mar 12:24]
haciendo la pregunta correcta en ingles obtenes la repuesta ya sea en
el foro de postgres (ya van dos respuestas alli, esta y la de los
triggers recursivos) o bien en stack, de primera pongo "postgres cant
delete row without aparente razon" y el primer resultado es "unable
delete a row" y alli lo pille en el instante, es pq un trigger before
llama una funcion con return NULL; y eso hace que el proceso haga skip
de todo lo que siga, una especie de break de tcl, y entonces el delete
no se haga.
** DONE trigger comprado (y otros) recursivo infinito al insertar
- State "DONE"       from "TODO"       [2019-05-15 mié 13:49]
* directivas
** hacer programas basicos y programasx
p.e. ventaspg y ventaspgx
ventaspg solo para pasar y controlar ventas apto para Rodrigo donde el
ver ventas para controlar tenga limite 100 y un ventaspgx donde
tengamos posibilidades mas extendidas que tenemos en el actual ventas,
como editar todas las tablas y cambiar mas cosas.
** hacer la menor carga posible al servidor para tener el menor delay posible
y el menor costo luego y la menor dependencia de demoras
** [2019-05-16 jue] a partir de hoy la directiva es aprender y desarrollar php/postgres
la idea es dejar tcl pq el nivel de dificultad de hacer un traspaso a
la red es bastante grande aunque el nivel de diseño y resultado
observado en una interface como la del pasado de ventas es fabuloso y
si aparte eso esta conectado en la nube es buenisimo, y eso me llevo
una semana de trabajo intenso. Estoy todavia muy dudoso en cuanto al
camino a emprender ambos caminos estan llenos de piedras y son muy
empindados y tienen enormes desafios.
** [2019-05-17 vie] vuelta atras la directiva es desarrollar TCL
** [2019-05-17 vie] decidimos que no habra compatibilidad con Windows
** [2019-05-20 lun] trabajo con una instancia google cloud en saopaulo
que tiene una latencia similar a una base local, para la mayoria de
las tareas basicas, quizas se note mas en una tarea dura de impresion
de listado o algo asi.
** [2019-05-25 sáb] seguir en TCL
* bugs o ideas particulares
** ventas
*** DONE [2019-05-20 lun] controlar que p=3 and ic=12
- State "DONE"       from "TODO"       [2019-05-20 lun 14:15]
se agrego una columna case when que controla que p=3 y cc=12 marque un
flag y ese flag lo coloreo, y ademas pongo la posibilidad de editar en
la misma planilla la p que antes no la tenia.
** fichas
*** tabla cuotas 
debo trabajar sobre una tabla cuotas enteramente funcional.
primero recuperemos la funcion gc-gct.
aca generamos una tabla cuotas que es igual a lo que nos entrega
cuotas sin recargo el recargo lo calcularemos in situ con los datos de
la planilla. esto permitiria con una sola consulta tener todos los
datos que necesitamos para emitir las planillas.
Una solucion al tema de la impresion seria manejar localmente los
datos de la impresion con sqlite::memory o sea bajar los datos con
postgres en una sola consulta y pasarlos a una tabla temporal de
sqlite y desde alli trabajar la impresion, eso puede llevar pocos
milisegundos
y de paso ser muy facil de manipular a nivel datos y a nivel manejo de
la estructura de datos desde esas tablas hacia el listado, y desde la
bajada de datos a la creacion de tablas tambien. pq todo se hace en
memoria.
Asi como en un segundo tablelist se lee 250 registros de ituizango y
te los pinta en un tablelist en la misma forma una tabla temporal de
sqlite te lo puede leer y desde alli mandar la impresion en forma
tradicional.
