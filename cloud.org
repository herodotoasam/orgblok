* TODO
** DONE generar un modelo en pgmodeler estilizado
- State "DONE"       from "TODO"       [2019-05-08 mié 15:04]
seria primero en 9.6 sin serial ni sequence solo primary key
eliminando los campos que no se usan.
generar el source en sql, editarlo y luego adosarlo en un escript con
pg_execute, para subirlo, con drops antes.
incluir los create functions.
Ya se hizo y funciona rapidamente, borra y recrea las tablas.
** DONE usaremos primarykey unicamente y generaremos el id por tcl
- State "DONE"       from "TODO"       [2019-05-09 jue 20:44]
las tablas sin el serial o el sequence permiten el llenado con csv en
el caso de barrios,zonas, en el caso de clientes dio un error ya en
los datos, que fue por datos de tipo money no aceptados en el formato
en que se presentaban.
El escript propio esta corriendo para clientes solo y va muy lento y
todavia no se si va a ser grabado. 
** [2019-05-08 mié] se produjo el pasado a mysql
al final con postgres tuvimos muchos inconvenientes a la hora de subir
los datos a cloud, (y tambien a la hora de pasar los datos de sqlite a
postgres convengamos)
** [2019-05-09 jue] mysql hasta ahora

*** creacion de instancia/usuario/db
*** lectura de dde por script tcl
*** script interno para generar tablas e importar csv (con errores)
*** subida a cloud de generacion de tablas por script tcl
*** importacion por csv (con errores) y por dump.
** TODO terminar de pasar las tablas para el dump
** TODO transcribir las funciones a mysql
** TODO transcribir el ventaspg.tcl a ventasmysql.tcl
* diary
** [2019-05-09 jue]
hoy un dia positivo dentro de todo pq he logrado algunos resultados
*** logre entrarle al ::mysql::tcl
casi de una pase el metodo llenarm y funciona incluso con una simpleza
mayor que los otros, un par de sentencias menos. es muy funcional el
wraper y le entre al map y al sel, por ahora.
*** pase casi directo sin problemas al llenado o volcado de datos de sqlite por tcl
casi se pasa directo lo que teniamos antes o sea que tendremos todo
rapido
me hizo renegar mucho el varchar limitado de clientes que quedaba
estrecho para los largos de campos, pero ya lo vamos a corregir.
*** como es el proceso
**** la tabla se crea rapidamente en workbech y de ahi se saca el sql y se lo pasa a tcl
**** en tcl se cura el script que sirve para crear tabla en localhost y en cloud
**** un script lee de sqlite3 los datos a localhost bastante rapido
**** por workbench hago un dump en un solo sql una sola transaccion instantaneo
**** ese solo archivo lo subo y lo importo en un toque en cloud
**** o sea no usaremos CSV sino tcl y dump, 
mucho mas rapido y manejable por mi, parte pq el csv es archivo por
archivo y el dump es general y el volcado lo hago por script y todo lo
que sea por escript por mas lerdo que sea se hace solo.
*** logre corregir el problema de las ñ 
que era un encoding que lo paso directo a tcl y me lee bien los datos.
*** Se podria decir que la estructura de trabajo ya esta lista, falta completar
falta terminar el resto de las tablas y subir todo junto y luego hacer
funcionar el pasador de ventas, y luego o antes quizas el tema de las
funciones.
** [2019-05-10 vie]
hoy un dia totalmente frustrante pq si bien es cierto que he logrado
ciertos avances que estaban cantados y eso en forma trabajosa veo
claroscuros en el programa y quedo totalmente trabado con el tema de
las funciones de forma tal que tengo que aprenderlo de cero y esto es
un tema crucial y muy engorroso para hacer y la funcion que
necesitamos es muy compleja.
Para poder hacer una prueba operativa del cloud tendria que poder
subir el sistema de archivos en la nube sin problemas y conectar el
cliente pasar ventas y poder pasar ventas sin problemas desde
cualquier maquina, En forma rapida y sin delay.
*** Al menos termino con el HITO tener todos los datos en la nube
tengo el script completo en tcl que permite rehacer las tablas
completamente en la nube por separado para evitar errores y ya lo voy
a unificar en localhost para tambien tener uno solo. Y emito por
workbench un solo dump que lo subo y lo lee sin problemas
rapidamente. O sea la subida de datos se haria sin complicaciones.
*** decido trabajar con el padron en forma local con sqlite3!!
mas alla de los problemas de mysql tambien en postgres era lerdo y por
lo tanto creo que no vale la pena. se puede tener una copia local en
toda maquina facilmente y consultar ahi. no hay problema con eso.
*** me queda por una parte el tedioso tema de estudiar funciones y triggers
*** por otro hacer la transcripcion del ventaspg a ventasmysql
eso hare mañana parte que es mas facil y lograble y parte pq es mas
util ya que lo que necesito es lograr saber lo antes posible si
podemos contar con un cliente que logre hacer algo mas complejo desde
el cloud. De ultima se podria hacer una pmovto boba que retorne la
fecha hasta que tengamos la verdadera.
